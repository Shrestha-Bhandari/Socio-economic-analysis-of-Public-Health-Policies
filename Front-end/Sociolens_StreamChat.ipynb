{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grO1e3bCcQPS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWNXznYsSF3H",
        "outputId": "9da20b7e-aab1-4350-f8d6-19dff0d7ed7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install streamlit transformers huggingface_hub peft torch datetime -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cscLebg9STph",
        "outputId": "65548edc-0e2a-4178-a2b4-6d6fbe30c242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.37.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.7-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.7-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.7\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bxscjg8SEyV",
        "outputId": "2d01b856-ac8b-4bf2-a4f9-fdd38fcd892d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer  # Add TextIteratorStreamer import\n",
        "from threading import Thread  # Add Thread import\n",
        "import re\n",
        "import random\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = \"YOUR_HF_API_KEY\"  # Replace with your Hugging Face token\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "# Load model and tokenizer\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    model_name = \"iyashnayi/SocioLens-llama-1\"  # Replace with your SocioLens-llama-1 model path\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# Conversational templates for greetings\n",
        "conversational_templates = {\n",
        "    r\"^(hi|hello|hey|greetings)(\\s.*)?$\": {\n",
        "        \"greetings\": [\"Hello\", \"Greetings\"],\n",
        "        \"status\": [\n",
        "            \"I am ready to assist with socio-economic analysis.\",\n",
        "            \"I am prepared to provide data-driven insights.\"\n",
        "        ],\n",
        "        \"offer\": [\n",
        "            \"How may I support your research today?\",\n",
        "            \"What socio-economic topic would you like to explore?\"\n",
        "        ],\n",
        "        \"combine\": lambda g, s, o: f\"{g}. {s} {o}\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def try_conversational_response(user_input: str) -> str | None:\n",
        "    for pattern, template in conversational_templates.items():\n",
        "        if re.match(pattern, user_input.lower()):\n",
        "            greeting = random.choice(template[\"greetings\"])\n",
        "            status = random.choice(template[\"status\"])\n",
        "            offer = random.choice(template[\"offer\"])\n",
        "            return template[\"combine\"](greeting, status, offer)\n",
        "    return None\n",
        "\n",
        "def get_prompt(user_input: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "    ### System Message:\n",
        "    You are SocioLens, a world-class research assistant specializing in socio-economic public health policy analysis. Your expertise lies in analyzing the interplay of economic, social, and health factors to provide actionable insights. Deliver responses that are clear, concise, and professional, avoiding fluff or jargon. Every answer must be grounded in logical reasoning, supported by relevant statistics, data, or credible sources when possible. If data is unavailable, acknowledge limitations transparently and provide informed estimates or qualitative analysis. Use a neutral, authoritative, and approachable tone.\n",
        "\n",
        "    ### Few-Shot Examples:\n",
        "    **User Input**: What is the impact of unemployment on public health in the U.S.?\n",
        "    **Response**: Unemployment in the U.S. correlates with increased mental health issues and reduced healthcare access. A 2020 study found that a 1% rise in unemployment increases depression rates by 0.8%. Uninsured rates rise, with 10% of unemployed adults losing coverage. This strains public health systems, increasing emergency care costs by $5 billion annually. Mitigation requires targeted job programs and expanded Medicaid.\n",
        "\n",
        "    **User Input**: How does income inequality affect life expectancy?\n",
        "    **Response**: Income inequality reduces life expectancy, particularly in high-inequality regions. A 2019 study showed that a 1% increase in the Gini coefficient correlates with a 0.5-year decrease in life expectancy for lower-income groups. In the U.S., the top 1% live 10-15 years longer than the bottom 1%. Policies like progressive taxation and universal healthcare can narrow this gap.\n",
        "\n",
        "    ### Instruction:\n",
        "    For complex queries, think step by step to ensure accuracy. For all responses, provide concise answers (50-100 words) backed by stats or reasoning. If unsure, state limitations clearly. Respond to the following user input:\n",
        "\n",
        "    **User Input**: {user_input}\n",
        "\n",
        "    ### Response:\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "def generate_text_stream(prompt: str, model, tokenizer, temperature: float = 0.0, top_p: float = 0.9, max_new_tokens: int = 512):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    generation_kwargs = {\n",
        "        \"input_ids\": inputs.input_ids,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": max(1e-8, temperature),  # Avoid zero for stability\n",
        "        \"top_p\": top_p,\n",
        "        \"do_sample\": temperature > 0 or top_p < 1.0,\n",
        "        \"streamer\": streamer,\n",
        "    }\n",
        "\n",
        "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "    thread.start()\n",
        "\n",
        "    full_reply = \"\"\n",
        "    for token in streamer:\n",
        "        if token:\n",
        "            full_reply += token\n",
        "            yield token\n",
        "    yield {\"full_reply\": full_reply}\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"SocioLens: Socio-Economic Public Health Analysis\")\n",
        "st.write(\"Ask about socio-economic factors, public health policies, or related topics.\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat history\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(f'<div class=\"bubble\">{message[\"content\"]}</div>', unsafe_allow_html=True)\n",
        "\n",
        "# User input\n",
        "if prompt := st.chat_input(\"Your question\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(f'<div class=\"bubble\">{prompt}</div>', unsafe_allow_html=True)\n",
        "\n",
        "    # Check for conversational response\n",
        "    conversational_response = try_conversational_response(prompt)\n",
        "    if conversational_response:\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(f'<div class=\"bubble\">{conversational_response}</div>', unsafe_allow_html=True)\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": conversational_response})\n",
        "    else:\n",
        "        # Generate assistant response\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                response_placeholder = st.empty()\n",
        "                full_reply = \"\"\n",
        "                for token in generate_text_stream(\n",
        "                    get_prompt(prompt),\n",
        "                    model,\n",
        "                    tokenizer,\n",
        "                    temperature=0.0,\n",
        "                    top_p=0.9,\n",
        "                    max_new_tokens=512\n",
        "                ):\n",
        "                    if isinstance(token, dict):\n",
        "                        full_reply = token[\"full_reply\"]\n",
        "                    else:\n",
        "                        full_reply += token\n",
        "                        response_placeholder.markdown(f'<div class=\"bubble\">{full_reply}</div>', unsafe_allow_html=True)\n",
        "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_reply})\n",
        "\n",
        "# Custom CSS for chat bubbles with darker colors\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        ".bubble {\n",
        "    padding: 10px;\n",
        "    border-radius: 10px;\n",
        "    background-color: #4a4a4a;  /* Dark gray for general bubble */\n",
        "    color: #ffffff;  /* White text for readability */\n",
        "}\n",
        ".chat-message.user .bubble {\n",
        "    background-color: #4682b4;  /* Steel blue for user messages */\n",
        "    color: #ffffff;\n",
        "}\n",
        ".chat-message.assistant .bubble {\n",
        "    background-color: #355e3b;  /* Dark green for assistant messages */\n",
        "    color: #ffffff;\n",
        "}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Erl8Del_SWI8"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill all existing tunnels\n",
        "ngrok.kill()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPnS-Mf4SX0C",
        "outputId": "1d1acc4a-12d8-459b-f49d-d627e1ff7c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Streamlit app is running at: NgrokTunnel: \"https://2692-34-124-134-87.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# Authenticate ngrok with your token\n",
        "!ngrok authtoken \"YOUR_NGROK_API_KEY\"  # Replace this with your actual ngrok token\n",
        "\n",
        "# Run Streamlit app in the background and expose it via ngrok\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Run Streamlit app (background execution)\n",
        "os.system(\"streamlit run app.py &\")\n",
        "\n",
        "# Open ngrok tunnel for the Streamlit app (exposing port 8501)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is running at: {public_url}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
