# -*- coding: utf-8 -*-
"""Pseudo_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Cqgx0TzQ1C9voVI2eRA0Efp83kq2-4K
"""

!nvidia-smi

!pip install -U bitsandbytes

!pip install -U transformers accelerate

!pip install torch torchvision

!pip install pandas openpyxl pymupdf
!pip install chromadb sentence-transformers
!pip install gradio fastapi uvicorn

from google.colab import drive
drive.mount('/content/drive')

!huggingface-cli login

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

model_name = "meta-llama/Llama-2-7b-hf"

# Configure 4-bit quantization to reduce memory usage
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map="auto")

print("LLaMA 2 loaded successfully!")

import pandas as pd
import glob

# Path where your CSV files are stored
csv_files = glob.glob("/content/drive/MyDrive/publichealth/dataframe/*.csv")  # Modify path as needed

# Read all CSV files into dataframes
dataframes = [pd.read_csv(file) for file in csv_files]

# Combine them into a single DataFrame
df = pd.concat(dataframes, ignore_index=True)

print("CSV data loaded successfully!")
print(df.head())  # Display first few rows

import fitz  # PyMuPDF
import os

pdf_folder = "/content/drive/MyDrive/publichealth/pdf"  # Change this to your actual folder path
pdf_data = []  # List to store extracted text

for pdf_file in os.listdir(pdf_folder):
    if pdf_file.endswith(".pdf"):  # Process only PDFs
        pdf_path = os.path.join(pdf_folder, pdf_file)
        doc = fitz.open(pdf_path)
        text = ""

        for page in doc:
            text += page.get_text() + "\n"

        # Store extracted text
        pdf_data.append({"file_name": pdf_file, "content": text})

print(f"Processed {len(pdf_data)} PDFs")
print(pdf_data[0]["content"][:500])  # Preview first PDF's text

import json

json_path = "pdf_data.json"

with open(json_path, "w", encoding="utf-8") as json_file:
    json.dump(pdf_data, json_file, ensure_ascii=False, indent=4)

print(f"JSON saved at {json_path}")

import fitz  # PyMuPDF
import glob

pdf_files = glob.glob("/content/drive/MyDrive/publichealth/pdf/*.pdf")  # Modify path as needed
policy_texts = {}

# Read PDFs and extract text
for file in pdf_files:
    doc = fitz.open(file)
    text = "\n".join([page.get_text() for page in doc])
    policy_texts[file] = text[:1000]  # Store only first 1000 chars for preview

print("Policy documents loaded successfully!")
print(policy_texts)

import chromadb

# Load JSON
with open("pdf_data.json", "r", encoding="utf-8") as json_file:
    pdf_data = json.load(json_file)

# Initialize ChromaDB
chroma_client = chromadb.PersistentClient(path="chroma_db")  # Persistent storage
collection = chroma_client.get_or_create_collection(name="pdf_collection")

# Add each PDF's content as a separate document
for idx, pdf in enumerate(pdf_data):
    collection.add(
        documents=[pdf["content"]],
        metadatas=[{"source": pdf["file_name"]}],
        ids=[f"doc_{idx+1}"]
    )

print(f"Added {len(pdf_data)} PDFs to ChromaDB")

from sentence_transformers import SentenceTransformer

# Load the embedding model
embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
def retrieve_documents(query):
    query_embedding = embedding_model.encode(query).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=2)
    return results["documents"][0] if results else "No relevant documents found."

query = "Unemployment increased among men in 2023"
retrieved_docs = retrieve_documents(query)
print("Retrieved documents:\n", retrieved_docs)

def generate_response(query):
    context = retrieve_documents(query)
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    return tokenizer.decode(output[0], skip_special_tokens=True)

query = "Unemployment increased among men in 2023"
response = generate_response(query)
print("Model Response:\n", response)